{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e91fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas as pd\n",
    "import torch\n",
    "import itertools\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91166769",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = open('Indian_Names.csv', 'r').read().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8dde9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(names)\n",
    "min(len(i) for i in names)\n",
    "max(len(i) for i in names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd30f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try to do this bigram stuff by ourselves first --> Only look at letter pairs and do a lookup.\n",
    "# Step 1: In the names dataset, get a frequency ordered list of all possible 2 letter combos\n",
    "substring_dict = {}\n",
    "\n",
    "for name in names:\n",
    "    for i in range(len(name)-1):\n",
    "        name_substring = name[i]+name[i+1]\n",
    "        if name_substring not in substring_dict:\n",
    "            substring_dict[name_substring]  = 1\n",
    "        else:\n",
    "            substring_dict[name_substring] += 1\n",
    "\n",
    "print(substring_dict)\n",
    "\n",
    "\n",
    "#Step 2: Let's try doing this w/o a neural net. We'll take a seed letter, a determined word length and \n",
    "# keep adding letters based on weighted probability\n",
    "\n",
    "min_name_length = min(len(i) for i in names)\n",
    "max_name_length = max(len(i) for i in names)\n",
    "seedstring = 'abcdefghijklmnopqrstuvwxyz'\n",
    "\n",
    "#Randomised inputs\n",
    "seed_letter = seedstring[random.randint(0,len(seedstring)-1)]\n",
    "word_length = random.randint(min_name_length, max_name_length)\n",
    "\n",
    "\n",
    "#Rather than a simple substring and frequency count, it would make sense to break the dictionary into further pieces like : \n",
    "# columns: first letter | next letter | frequency | normalized probability \n",
    "optimized_substring_dict = {}\n",
    "for bigram_str, count in substring_dict.items():\n",
    "    first_letter = bigram_str[0]\n",
    "    next_letter = bigram_str[1]\n",
    "\n",
    "    if first_letter not in optimized_substring_dict:\n",
    "        optimized_substring_dict[first_letter] = []\n",
    "    \n",
    "    optimized_substring_dict[first_letter].append((next_letter, count))\n",
    "\n",
    "for first_letter in optimized_substring_dict:\n",
    "    total_count = sum(count for _, count in optimized_substring_dict[first_letter])\n",
    "\n",
    "    optimized_substring_dict[first_letter] = [\n",
    "        (next_letter, count, count/total_count)\n",
    "        for next_letter, count in optimized_substring_dict[first_letter]\n",
    "    ]\n",
    "\n",
    "print(optimized_substring_dict['a'])\n",
    "\n",
    "#Function to generate a name given a seed letter, a word length and a target dictionary\n",
    "def generate_name(seed_letter, word_length, target_dictionary):\n",
    "    name = seed_letter\n",
    "    current_letter = seed_letter\n",
    "\n",
    "    for i in range (word_length-1):\n",
    "        if current_letter not in target_dictionary:\n",
    "            break\n",
    "\n",
    "        next_options = target_dictionary[current_letter]\n",
    "\n",
    "        letters = [option[0] for option in next_options]\n",
    "        probabilities = [option[2] for option in next_options]\n",
    "\n",
    "        next_letter = random.choices(letters, weights = probabilities)[0]\n",
    "\n",
    "        name += next_letter\n",
    "        current_letter = next_letter\n",
    "    \n",
    "    return name\n",
    "\n",
    "generate_name(seed_letter, word_length, optimized_substring_dict)\n",
    "\n",
    "        \n",
    "for _ in range(10):\n",
    "    seed_letter = seedstring[random.randint(0,len(seedstring)-1)]\n",
    "    word_length = random.randint(min_name_length, max_name_length)\n",
    "    name = generate_name(seed_letter, word_length, optimized_substring_dict)\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40505742",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lmao that didn't work too great. So let's now look at how Andrej does it. \n",
    "# Difference 1: for bigrams, he's added a start and end token for each name.\n",
    "\n",
    "bigram_dictionary = {}\n",
    "\n",
    "for name in names:\n",
    "    #Add in start and end characters for each name\n",
    "    full_charlist = ['<S>'] + list(name) + ['<E>']\n",
    "    for char1, char2 in zip(full_charlist, full_charlist[1:]):\n",
    "        bigram = (char1, char2)\n",
    "        bigram_dictionary[bigram] = bigram_dictionary.get(bigram, 0) + 1\n",
    "        print (char1, char2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496e2a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted(bigram_dictionary.items(), key = lambda kv: -kv[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd94eb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Better to handle this in pytorch than dictionaries\n",
    "#There are 28x28 potential bigrams (26 letters + 2 special start/end chars)\n",
    "\n",
    "\n",
    "#Let's get the list of all chars in the bigram_dictionary\n",
    "import torch\n",
    "names_array = torch.zeros((27,27), dtype=torch.int32)\n",
    "\n",
    "character_list = sorted(list(set(''.join(names))))\n",
    "string_to_int = []\n",
    "string_to_int = {s:i+1 for i, s in enumerate(character_list)}\n",
    "string_to_int['.']=0\n",
    "\n",
    "print(string_to_int)\n",
    "\n",
    "for name in names:\n",
    "    chars = ['.']+list(name)+['.']\n",
    "    for char1, char2 in zip(chars, chars[1:]):\n",
    "        index1 = string_to_int[char1]\n",
    "        index2 = string_to_int[char2]\n",
    "        names_array[index1, index2] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3505ba12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.imshow(names_array)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b054d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Want a better visualization? Don't really need it but let's go I guess lol\n",
    "\n",
    "#string_to_int becomes int_to_string here\n",
    "\n",
    "int_to_string = {i:s for s,i in string_to_int.items()}\n",
    "int_to_string\n",
    "\n",
    "%matplotlib inline\n",
    "plt.figure(figsize = (16,16))\n",
    "plt.imshow(names_array, cmap = 'Reds')\n",
    "for i in range(27):\n",
    "    for j in range(27):\n",
    "        chstr = int_to_string[i] + int_to_string[j]\n",
    "        plt.text(j, i, chstr, ha = 'center', va='bottom', color='gray')\n",
    "        plt.text(j,i, names_array[i,j].item(), ha = 'center', va='top', color='gray')\n",
    "plt.axis('off')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09ba78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "names_array[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "9fc161ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sh\n",
      "jelohatoegirelapa\n",
      "pha\n",
      "ellele\n",
      "den\n",
      "m\n",
      "desen\n",
      "ie\n",
      "mawisopeeeni\n",
      "rllude\n"
     ]
    }
   ],
   "source": [
    "generated_name = []\n",
    "ix = 0\n",
    "generation = torch.Generator().manual_seed(random.randint(0,991934359))\n",
    "\n",
    "#Now we have it normalized. In our method, we used random.choice() using probabilities. Pytorch has multinomial\n",
    "#rather than recalculating and renormalizing the arrays everytime, let's create a full array holding all this at once. (check the commented lines)\n",
    "\n",
    "Final_prob_array = (names_array+1).float()\n",
    "Final_prob_array /= Final_prob_array.sum(1, keepdim=True)\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    while True:\n",
    "        probability_array = Final_prob_array[ix]    \n",
    "        ix = torch.multinomial(probability_array, num_samples=1, replacement=True, generator=generation).item()\n",
    "        generated_name.append(int_to_string[ix])\n",
    "        if ix == 0:\n",
    "            break\n",
    "\n",
    "    print(''.join(generated_name[:-1]))\n",
    "    generated_name = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef04569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now try to evaluate the quality of this \"model\". How can we take our data and define a loss function to optimize on?\n",
    "\n",
    "#Actually could be fun to take the 27x27 as our entire neural network and sort of try to make it overfit our data. \n",
    "# Say we start with these as our biases. \n",
    "# But what about the weights?\n",
    "\n",
    "\n",
    "# Measuring quality\n",
    "\n",
    "# If we picked totally at random, the chance of any bigram pair should be 1/27 ~ 0.037\n",
    "# But as we can see, the model is assigning higher than 0.037 probability to many pairs in the training set, but not all.\n",
    "# Ideally, in a perfect world, the model would assign 1 to every bigram pair shown in the training dataset.\n",
    "# If we were to take one number to measure the quality of this model - from statistical theory - it's called likelihood\n",
    "# Likelihood is the product of all these probabilities. It should be as high as possible. But these numbers make it unwieldy\n",
    "# So we use log likelihood. But log likelihood here is negative, and the closer to zero it is the better for us. \n",
    "# So we'll invert it, and for a loss function we use the average of the negative log likelihood\n",
    "\n",
    "log_likelihood = 0.0\n",
    "n=0\n",
    "for name in names:\n",
    "    chars = ['.']+list(name)+['.']\n",
    "    for char1, char2 in zip(chars, chars[1:]):\n",
    "        index1 = string_to_int[char1]\n",
    "        index2 = string_to_int[char2]\n",
    "        prob = Final_prob_array[index1, index2]\n",
    "        log_prob = torch.log(prob)\n",
    "        log_likelihood += log_prob\n",
    "        n += 1\n",
    "        print(f'{char1}{char2}: {prob:.3f} {log_prob:.3f}')\n",
    "\n",
    "print(f'{log_likelihood=}')\n",
    "negative_log_likelihood = -log_likelihood\n",
    "print(f'{negative_log_likelihood = }')\n",
    "avg_nll = negative_log_likelihood/n\n",
    "print(f'{avg_nll:.3f}')\n",
    "\n",
    "#you can just enter a name here and check. my name was around 2.3, so not too bad. within distribution types\n",
    "#If you enter nonsensical stuff like 'ahkqxsd', the loss becomes infinity since qx isn't in the distribution\n",
    "# To clean this, people do model smoothing, viz. add some small number like 1 to all the occurrences across the bigram frequency distribution\n",
    "# The more you add, the \"smoother\" your model gets, obviously.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d8b479",
   "metadata": {},
   "source": [
    "# Doing this with neural nets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "1530c86a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples 253317\n"
     ]
    }
   ],
   "source": [
    "# We now want to do this with neural nets. We know the training data - bigrams from our examples.\n",
    "# Let's create the training data (x,y)\n",
    "\n",
    "xs, ys = [], []\n",
    "\n",
    "for name in names:\n",
    "    chars = ['.'] + list(name) + ['.']\n",
    "    for char1, char2 in zip(chars, chars[1:]):\n",
    "        index1 = string_to_int[char1]\n",
    "        index2 = string_to_int[char2]\n",
    "        #print(char1, char2)\n",
    "        xs.append(index1)\n",
    "        ys.append(index2)\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "num = xs.nelement()\n",
    "print('number of examples', num)\n",
    "#xs is our preceding letter in the bigram and ys is the succeeding letter. So our NN should know that following 1: 1,2,14 should have high probabilities\n",
    "#Feeding in integer values into an NN is done via one hot encoding\n",
    "\n",
    "import torch.nn.functional as F\n",
    "x_enc = F.one_hot(xs, num_classes=27).float()\n",
    "#print(x_enc)\n",
    "\n",
    "#Remember to cast one hot encoding of xs to float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe071a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.1249e-01,  1.4354e-01, -7.2369e-01,  ..., -2.0935e-01,\n",
       "          6.0515e-01, -8.6556e-01],\n",
       "        [ 3.2015e-01,  3.0683e-02, -9.9661e-01,  ..., -4.3654e-02,\n",
       "         -1.1543e+00, -1.4847e+00],\n",
       "        [ 3.2015e-01,  3.0683e-02, -9.9661e-01,  ..., -4.3654e-02,\n",
       "         -1.1543e+00, -1.4847e+00],\n",
       "        ...,\n",
       "        [ 3.6177e-01, -9.7197e-01,  1.0948e-01,  ..., -1.7869e-03,\n",
       "         -7.5519e-01,  1.1843e+00],\n",
       "        [-3.0267e-01,  1.6177e+00, -1.0565e+00,  ...,  9.3357e-01,\n",
       "         -1.1384e+00,  1.4911e+00],\n",
       "        [ 7.4620e-01, -6.9121e-01, -1.9286e+00,  ..., -8.5729e-01,\n",
       "         -3.8481e-01, -8.2572e-02]])"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's initialize randomized weights\n",
    "W = torch.randn((27,27))\n",
    "x_enc @ W\n",
    "\n",
    "#x_enc @ W is the matrix multiplication in pytorch\n",
    "print(f'W.shape is {W.shape}, x_enc.shape is {x_enc.shape}')\n",
    "\n",
    "# x_enc @ W is 6, 27 * 27,27 = 6,27 matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "0335e040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "Bigram example 1: .a (indexes 0, 1)\n",
      "input to the neural net: 0\n",
      "Output probabilities from the neural net: tensor([0.0221, 0.0315, 0.0132, 0.0483, 0.1499, 0.0171, 0.1027, 0.0530, 0.0614,\n",
      "        0.0098, 0.0191, 0.0056, 0.0421, 0.0454, 0.0647, 0.0091, 0.0100, 0.0113,\n",
      "        0.0322, 0.0191, 0.0483, 0.0257, 0.0515, 0.0233, 0.0221, 0.0500, 0.0115])\n",
      "Label: Actual next character: 1\n",
      "Probability assigned by the neural net to the actual next character: 0.03151710703969002\n",
      "Log likelihood: -3.4572248458862305\n",
      "Negative log likelihood: 3.4572248458862305\n",
      "====================\n",
      "average negative log likelihood, i.e  loss =  0.5762041211128235\n",
      "---------------------------------\n",
      "Bigram example 2: aa (indexes 1, 1)\n",
      "input to the neural net: 1\n",
      "Output probabilities from the neural net: tensor([0.0381, 0.0285, 0.0102, 0.1508, 0.0055, 0.0577, 0.0089, 0.0484, 0.0197,\n",
      "        0.0173, 0.1361, 0.0250, 0.0114, 0.0644, 0.0453, 0.0256, 0.0350, 0.0720,\n",
      "        0.0289, 0.0104, 0.0041, 0.0395, 0.0037, 0.0719, 0.0265, 0.0087, 0.0063])\n",
      "Label: Actual next character: 1\n",
      "Probability assigned by the neural net to the actual next character: 0.028537796810269356\n",
      "Log likelihood: -3.556525945663452\n",
      "Negative log likelihood: 3.556525945663452\n",
      "====================\n",
      "average negative log likelihood, i.e  loss =  1.1689585447311401\n",
      "---------------------------------\n",
      "Bigram example 3: ab (indexes 1, 2)\n",
      "input to the neural net: 1\n",
      "Output probabilities from the neural net: tensor([0.0381, 0.0285, 0.0102, 0.1508, 0.0055, 0.0577, 0.0089, 0.0484, 0.0197,\n",
      "        0.0173, 0.1361, 0.0250, 0.0114, 0.0644, 0.0453, 0.0256, 0.0350, 0.0720,\n",
      "        0.0289, 0.0104, 0.0041, 0.0395, 0.0037, 0.0719, 0.0265, 0.0087, 0.0063])\n",
      "Label: Actual next character: 2\n",
      "Probability assigned by the neural net to the actual next character: 0.010215832851827145\n",
      "Log likelihood: -4.5838165283203125\n",
      "Negative log likelihood: 4.5838165283203125\n",
      "====================\n",
      "average negative log likelihood, i.e  loss =  1.9329279661178589\n",
      "---------------------------------\n",
      "Bigram example 4: ba (indexes 2, 1)\n",
      "input to the neural net: 2\n",
      "Output probabilities from the neural net: tensor([0.0093, 0.0367, 0.0315, 0.0936, 0.0103, 0.0093, 0.0120, 0.0053, 0.0035,\n",
      "        0.0073, 0.0122, 0.0415, 0.0155, 0.0270, 0.0237, 0.2231, 0.2205, 0.0582,\n",
      "        0.0223, 0.0177, 0.0144, 0.0105, 0.0143, 0.0348, 0.0013, 0.0357, 0.0085])\n",
      "Label: Actual next character: 1\n",
      "Probability assigned by the neural net to the actual next character: 0.036726176738739014\n",
      "Log likelihood: -3.3042654991149902\n",
      "Negative log likelihood: 3.3042654991149902\n",
      "====================\n",
      "average negative log likelihood, i.e  loss =  2.4836387634277344\n",
      "---------------------------------\n",
      "Bigram example 5: an (indexes 1, 14)\n",
      "input to the neural net: 1\n",
      "Output probabilities from the neural net: tensor([0.0381, 0.0285, 0.0102, 0.1508, 0.0055, 0.0577, 0.0089, 0.0484, 0.0197,\n",
      "        0.0173, 0.1361, 0.0250, 0.0114, 0.0644, 0.0453, 0.0256, 0.0350, 0.0720,\n",
      "        0.0289, 0.0104, 0.0041, 0.0395, 0.0037, 0.0719, 0.0265, 0.0087, 0.0063])\n",
      "Label: Actual next character: 14\n",
      "Probability assigned by the neural net to the actual next character: 0.04530072212219238\n",
      "Log likelihood: -3.0944323539733887\n",
      "Negative log likelihood: 3.0944323539733887\n",
      "====================\n",
      "average negative log likelihood, i.e  loss =  2.999377489089966\n",
      "---------------------------------\n",
      "Bigram example 6: n. (indexes 14, 0)\n",
      "input to the neural net: 14\n",
      "Output probabilities from the neural net: tensor([0.0809, 0.1698, 0.0045, 0.0163, 0.0495, 0.0219, 0.0224, 0.0146, 0.0073,\n",
      "        0.0147, 0.0299, 0.0509, 0.0458, 0.0185, 0.0651, 0.0114, 0.0509, 0.0558,\n",
      "        0.0335, 0.0171, 0.0253, 0.0859, 0.0650, 0.0197, 0.0060, 0.0122, 0.0052])\n",
      "Label: Actual next character: 0\n",
      "Probability assigned by the neural net to the actual next character: 0.08093664050102234\n",
      "Log likelihood: -2.5140886306762695\n",
      "Negative log likelihood: 2.5140886306762695\n",
      "====================\n",
      "average negative log likelihood, i.e  loss =  3.4183924198150635\n"
     ]
    }
   ],
   "source": [
    "nlls = torch.zeros(6)\n",
    "\n",
    "for i in range(6):\n",
    "    x = xs[i].item()\n",
    "    y = ys[i].item()\n",
    "    print('---------------------------------')\n",
    "    print(f'Bigram example {i+1}: {int_to_string[x]}{int_to_string[y]} (indexes {x}, {y})')\n",
    "    print(f'input to the neural net: {x}')\n",
    "    print(f'Output probabilities from the neural net: {probs[i]}')\n",
    "    print(f'Label: Actual next character: {y}')\n",
    "    p = probs[i,y]\n",
    "    print(f'Probability assigned by the neural net to the actual next character: {p.item()}')\n",
    "    logp = torch.log(p)\n",
    "    print(f'Log likelihood: {logp.item()}')\n",
    "    nll = -logp\n",
    "    print(f'Negative log likelihood: {nll.item()}')\n",
    "    nlls[i] = nll\n",
    "\n",
    "    print('====================')\n",
    "    print('average negative log likelihood, i.e  loss = ',nlls.mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "4cbfa484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.902346134185791\n",
      "3.4767067432403564\n",
      "3.25004506111145\n",
      "3.10648250579834\n",
      "3.001192092895508\n",
      "2.9214439392089844\n",
      "2.8598499298095703\n",
      "2.811532974243164\n",
      "2.7730560302734375\n",
      "2.741954803466797\n",
      "2.716430187225342\n",
      "2.6951582431793213\n",
      "2.6771771907806396\n",
      "2.661797523498535\n",
      "2.6485068798065186\n",
      "2.6369099617004395\n",
      "2.6266942024230957\n",
      "2.6176159381866455\n",
      "2.609485626220703\n",
      "2.602159023284912\n",
      "2.595524549484253\n",
      "2.589493989944458\n",
      "2.583996057510376\n",
      "2.5789716243743896\n",
      "2.574369430541992\n",
      "2.5701465606689453\n",
      "2.5662639141082764\n",
      "2.562687397003174\n",
      "2.55938720703125\n",
      "2.5563361644744873\n",
      "2.5535106658935547\n",
      "2.550889015197754\n",
      "2.5484516620635986\n",
      "2.5461816787719727\n",
      "2.5440633296966553\n",
      "2.5420825481414795\n",
      "2.5402274131774902\n",
      "2.5384867191314697\n",
      "2.5368499755859375\n",
      "2.535308837890625\n",
      "2.5338544845581055\n",
      "2.5324807167053223\n",
      "2.5311803817749023\n",
      "2.5299479961395264\n",
      "2.528777837753296\n",
      "2.527665615081787\n",
      "2.5266077518463135\n",
      "2.525599241256714\n",
      "2.524637460708618\n",
      "2.5237185955047607\n",
      "2.5228402614593506\n",
      "2.5219995975494385\n",
      "2.5211946964263916\n",
      "2.5204226970672607\n",
      "2.5196821689605713\n",
      "2.5189707279205322\n",
      "2.518286943435669\n",
      "2.517629384994507\n",
      "2.516995906829834\n",
      "2.516386032104492\n",
      "2.5157978534698486\n",
      "2.515230655670166\n",
      "2.514683246612549\n",
      "2.5141546726226807\n",
      "2.513643741607666\n",
      "2.5131494998931885\n",
      "2.512671709060669\n",
      "2.5122087001800537\n",
      "2.511760711669922\n",
      "2.511326551437378\n",
      "2.5109052658081055\n",
      "2.5104973316192627\n",
      "2.510101079940796\n",
      "2.509716033935547\n",
      "2.5093424320220947\n",
      "2.508979558944702\n",
      "2.508626699447632\n",
      "2.5082836151123047\n",
      "2.5079500675201416\n",
      "2.5076253414154053\n",
      "2.5073089599609375\n",
      "2.5070016384124756\n",
      "2.5067014694213867\n",
      "2.5064096450805664\n",
      "2.506124973297119\n",
      "2.505847692489624\n",
      "2.5055770874023438\n",
      "2.5053131580352783\n",
      "2.5050556659698486\n",
      "2.5048043727874756\n",
      "2.50455904006958\n",
      "2.504319190979004\n",
      "2.5040853023529053\n",
      "2.503856897354126\n",
      "2.5036330223083496\n",
      "2.503415107727051\n",
      "2.503201484680176\n",
      "2.502992868423462\n",
      "2.502788782119751\n",
      "2.502588987350464\n",
      "2.5023934841156006\n",
      "2.502202272415161\n",
      "2.5020153522491455\n",
      "2.5018322467803955\n",
      "2.501652479171753\n",
      "2.5014772415161133\n",
      "2.501304864883423\n",
      "2.501136302947998\n",
      "2.5009710788726807\n",
      "2.50080943107605\n",
      "2.500650644302368\n",
      "2.500495195388794\n",
      "2.500342845916748\n",
      "2.5001931190490723\n",
      "2.500046491622925\n",
      "2.4999024868011475\n",
      "2.4997613430023193\n",
      "2.4996228218078613\n",
      "2.4994871616363525\n",
      "2.4993536472320557\n",
      "2.4992222785949707\n",
      "2.499094009399414\n",
      "2.4989676475524902\n",
      "2.4988439083099365\n",
      "2.4987218379974365\n",
      "2.4986021518707275\n",
      "2.4984848499298096\n",
      "2.4983694553375244\n",
      "2.498255968093872\n",
      "2.4981443881988525\n",
      "2.498034715652466\n",
      "2.497926950454712\n",
      "2.4978208541870117\n",
      "2.4977169036865234\n",
      "2.4976143836975098\n",
      "2.49751353263855\n",
      "2.4974145889282227\n",
      "2.49731707572937\n",
      "2.4972214698791504\n",
      "2.497126817703247\n",
      "2.4970343112945557\n",
      "2.4969427585601807\n",
      "2.4968531131744385\n",
      "2.4967644214630127\n",
      "2.4966773986816406\n",
      "2.496591567993164\n",
      "2.496507167816162\n",
      "2.4964237213134766\n",
      "2.4963414669036865\n",
      "2.4962611198425293\n",
      "2.4961817264556885\n",
      "2.496103525161743\n",
      "2.4960265159606934\n",
      "2.495950222015381\n",
      "2.495875835418701\n",
      "2.4958016872406006\n",
      "2.4957292079925537\n",
      "2.495657205581665\n",
      "2.495586633682251\n",
      "2.4955170154571533\n",
      "2.495448589324951\n",
      "2.4953806400299072\n",
      "2.495313882827759\n",
      "2.4952480792999268\n",
      "2.495182991027832\n",
      "2.495119333267212\n",
      "2.495055913925171\n",
      "2.4949936866760254\n",
      "2.494931936264038\n",
      "2.4948716163635254\n",
      "2.4948112964630127\n",
      "2.4947524070739746\n",
      "2.494694232940674\n",
      "2.4946367740631104\n",
      "2.4945802688598633\n",
      "2.494523763656616\n",
      "2.4944686889648438\n",
      "2.4944138526916504\n",
      "2.4943599700927734\n",
      "2.494306802749634\n",
      "2.4942543506622314\n",
      "2.4942023754119873\n",
      "2.4941511154174805\n",
      "2.494100570678711\n",
      "2.4940502643585205\n",
      "2.4940006732940674\n",
      "2.4939522743225098\n",
      "2.493903875350952\n",
      "2.4938559532165527\n",
      "2.4938089847564697\n",
      "2.493762493133545\n",
      "2.4937164783477783\n",
      "2.49367094039917\n",
      "2.493626356124878\n",
      "2.493581771850586\n",
      "2.493537664413452\n",
      "2.4934942722320557\n",
      "2.4934515953063965\n",
      "2.4934091567993164\n",
      "2.4933671951293945\n",
      "2.493325710296631\n",
      "2.4932849407196045\n",
      "2.493244171142578\n",
      "2.493204355239868\n",
      "2.4931647777557373\n",
      "2.4931254386901855\n",
      "2.493086576461792\n",
      "2.4930479526519775\n",
      "2.4930102825164795\n",
      "2.4929728507995605\n",
      "2.4929356575012207\n",
      "2.492898464202881\n",
      "2.4928624629974365\n",
      "2.492826223373413\n",
      "2.492790699005127\n",
      "2.49275541305542\n",
      "2.492720603942871\n",
      "2.4926857948303223\n",
      "2.4926514625549316\n",
      "2.49261736869812\n",
      "2.492584466934204\n",
      "2.49255108833313\n",
      "2.4925179481506348\n",
      "2.492485761642456\n",
      "2.4924535751342773\n",
      "2.492421865463257\n",
      "2.4923899173736572\n",
      "2.492358684539795\n",
      "2.4923276901245117\n",
      "2.4922971725463867\n",
      "2.4922666549682617\n",
      "2.492236614227295\n",
      "2.492206573486328\n",
      "2.4921772480010986\n",
      "2.492147922515869\n",
      "2.4921188354492188\n",
      "2.4920904636383057\n",
      "2.4920616149902344\n",
      "2.4920334815979004\n",
      "2.4920055866241455\n",
      "2.4919776916503906\n",
      "2.491950273513794\n",
      "2.4919233322143555\n",
      "2.491895914077759\n",
      "2.4918696880340576\n",
      "2.4918429851531982\n",
      "2.491816997528076\n",
      "2.491791009902954\n",
      "2.491765022277832\n",
      "2.491739273071289\n",
      "2.4917142391204834\n",
      "2.4916889667510986\n",
      "2.491664171218872\n",
      "2.4916393756866455\n",
      "2.491614818572998\n",
      "2.4915902614593506\n",
      "2.4915664196014404\n",
      "2.4915428161621094\n",
      "2.491518497467041\n",
      "2.491495370864868\n",
      "2.491471767425537\n",
      "2.4914488792419434\n",
      "2.4914259910583496\n",
      "2.491403102874756\n",
      "2.491380453109741\n",
      "2.4913578033447266\n",
      "2.49133563041687\n",
      "2.491313934326172\n",
      "2.4912919998168945\n",
      "2.4912703037261963\n",
      "2.491248846054077\n",
      "2.491227626800537\n",
      "2.491206407546997\n",
      "2.491185188293457\n",
      "2.491164207458496\n",
      "2.4911437034606934\n",
      "2.4911229610443115\n",
      "2.491102933883667\n",
      "2.4910826683044434\n",
      "2.491062641143799\n",
      "2.491042375564575\n",
      "2.4910225868225098\n",
      "2.4910032749176025\n",
      "2.4909839630126953\n",
      "2.490964651107788\n",
      "2.4909451007843018\n",
      "2.4909260272979736\n",
      "2.4909071922302246\n",
      "2.4908883571624756\n",
      "2.4908699989318848\n",
      "2.490851402282715\n",
      "2.490832805633545\n",
      "2.4908149242401123\n",
      "2.4907965660095215\n",
      "2.490778684616089\n",
      "2.490760564804077\n",
      "2.4907429218292236\n",
      "2.49072527885437\n",
      "2.4907078742980957\n",
      "2.490690231323242\n",
      "2.490673303604126\n",
      "2.4906558990478516\n",
      "2.4906389713287354\n",
      "2.490622043609619\n",
      "2.490605592727661\n",
      "2.490588903427124\n",
      "2.490571975708008\n",
      "2.49055552482605\n",
      "2.490539312362671\n",
      "2.490523099899292\n",
      "2.490506887435913\n",
      "2.4904909133911133\n",
      "2.4904749393463135\n",
      "2.490459442138672\n",
      "2.490443468093872\n",
      "2.4904279708862305\n",
      "2.490412473678589\n",
      "2.4903972148895264\n",
      "2.490381956100464\n",
      "2.4903664588928223\n",
      "2.490351438522339\n",
      "2.4903364181518555\n",
      "2.490321397781372\n",
      "2.4903066158294678\n",
      "2.4902920722961426\n",
      "2.4902775287628174\n",
      "2.490262746810913\n",
      "2.490248680114746\n",
      "2.490234136581421\n",
      "2.490220069885254\n",
      "2.490205764770508\n",
      "2.490191698074341\n",
      "2.4901773929595947\n",
      "2.490163803100586\n",
      "2.490149736404419\n",
      "2.49013614654541\n",
      "2.4901225566864014\n",
      "2.4901089668273926\n",
      "2.490095376968384\n",
      "2.490082025527954\n",
      "2.490068197250366\n",
      "2.490055561065674\n",
      "2.4900424480438232\n",
      "2.4900293350219727\n",
      "2.490015983581543\n",
      "2.4900031089782715\n",
      "2.489990234375\n",
      "2.4899773597717285\n",
      "2.4899649620056152\n",
      "2.489952325820923\n",
      "2.4899394512176514\n",
      "2.489927053451538\n",
      "2.489914655685425\n",
      "2.4899024963378906\n",
      "2.4898900985717773\n",
      "2.489877700805664\n",
      "2.489865779876709\n",
      "2.489853620529175\n",
      "2.4898412227630615\n",
      "2.4898297786712646\n",
      "2.4898178577423096\n",
      "2.4898059368133545\n",
      "2.4897944927215576\n",
      "2.4897825717926025\n",
      "2.4897708892822266\n",
      "2.4897594451904297\n",
      "2.489748001098633\n",
      "2.489736318588257\n",
      "2.489725351333618\n",
      "2.4897139072418213\n",
      "2.4897029399871826\n",
      "2.489691734313965\n",
      "2.4896810054779053\n",
      "2.4896695613861084\n",
      "2.4896585941314697\n",
      "2.489647388458252\n",
      "2.4896366596221924\n",
      "2.489626169204712\n",
      "2.4896152019500732\n",
      "2.4896047115325928\n",
      "2.489593744277954\n",
      "2.4895834922790527\n",
      "2.489572763442993\n",
      "2.4895622730255127\n",
      "2.4895520210266113\n",
      "2.48954176902771\n",
      "2.4895312786102295\n",
      "2.4895212650299072\n",
      "2.489511251449585\n",
      "2.4895007610321045\n",
      "2.4894907474517822\n",
      "2.489480495452881\n",
      "2.4894707202911377\n",
      "2.4894611835479736\n",
      "2.4894511699676514\n",
      "2.489441156387329\n",
      "2.489431619644165\n",
      "2.489422082901001\n",
      "2.489412307739258\n",
      "2.4894022941589355\n"
     ]
    }
   ],
   "source": [
    "#randomly initialize  27 neurons weights. each neuron receives 27 inputs\n",
    "generation = torch.Generator().manual_seed(214782235)\n",
    "W = torch.randn((27, 27), generator = generation, requires_grad=True)\n",
    "\n",
    "for k in range(400):\n",
    "    # forward pass\n",
    "    xenc = F.one_hot(xs, num_classes=27).float()\n",
    "    logits = xenc @ W\n",
    "    counts = logits.exp()\n",
    "    probs = counts/counts.sum(1, keepdims=True)\n",
    "    #Loss\n",
    "    loss_funk = -probs[torch.arange(num), ys].log().mean()\n",
    "    print(loss_funk.item())\n",
    "\n",
    "    #Backward pass\n",
    "    W.grad = None\n",
    "    loss_funk.backward()\n",
    "\n",
    "    #Parameter update\n",
    "    W.data += -50*W.grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c712a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "araida.\n",
      "stranomizacharajhanamano.\n",
      "jire.\n",
      "diesqud.\n",
      "ndryi.\n",
      "rlaniriavquiyaya.\n",
      "deniraliaba.\n",
      "z.\n",
      "juanai.\n",
      "jrlaitviana.\n"
     ]
    }
   ],
   "source": [
    "#Final check. Let's sample the neural net model as well.\n",
    "\n",
    "for i in range(10):\n",
    "    out = []\n",
    "    index = 0\n",
    "    while True:\n",
    "        xenc = F.one_hot(torch.tensor([index]), num_classes = 27).float()\n",
    "        logits = xenc @ W\n",
    "        counts = logits.exp()\n",
    "\n",
    "        p = counts/counts.sum(1, keepdim=True)\n",
    "\n",
    "        index = torch.multinomial(p, num_samples=1, replacement=True, generator=generation).item()\n",
    "        out.append(int_to_string[index])\n",
    "\n",
    "        if index == 0:\n",
    "            break\n",
    "    print(''.join(out))\n",
    "\n",
    "    #Seems good to go. Equally nonsensical name outputs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
